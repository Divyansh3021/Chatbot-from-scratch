{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return [line.strip() for line in lines]\n",
    "\n",
    "dialogues = load_data(\"../dataset/train/dialogues_train.txt\")\n",
    "emotions = load_data(\"../dataset/train/dialogues_emotion_train.txt\")\n",
    "dialog_acts = load_data(\"../dataset/train/dialogues_act_train.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data formatting\n",
    "input_sequences = []\n",
    "target_sequences = []\n",
    "emotion_labels = []\n",
    "dialog_act_labels = []\n",
    "\n",
    "for dialogue, emotions_line, dialog_act in zip(dialogues, emotions, dialog_acts):\n",
    "    user_utterances = dialogue.split('__eou__')[:-1]  # Split by end-of-utterance marker\n",
    "    num_utterances = len(user_utterances)\n",
    "\n",
    "    # Handle emotions\n",
    "    individual_emotions = emotions_line.split()  # Split emotions on spaces\n",
    "    emotion_label = int(individual_emotions[0])  # Take the first emotion label\n",
    "\n",
    "    # Handle dialog acts\n",
    "    individual_dialog_acts = dialog_act.split()\n",
    "    dialog_act_label = int(individual_dialog_acts[0])\n",
    "\n",
    "    for i in range(num_utterances - 1):\n",
    "        input_seq = f\"User: {user_utterances[i]}\"\n",
    "        target_seq = f\"Chatbot: {user_utterances[i+1]}\"\n",
    "        input_sequences.append(input_seq)\n",
    "        target_sequences.append(target_seq)\n",
    "        emotion_labels.append(emotion_label)  # Use the extracted emotion label\n",
    "        dialog_act_labels.append(dialog_act_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Embedding, PositionalEncoding, MultiHeadAttention, FeedForwardNetwork, LayerNormalization, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(max_seq_len, d_model):\n",
    "    positions = tf.range(max_seq_len, dtype=tf.float32)[:, tf.newaxis]\n",
    "    angles = 1 / tf.pow(10000, (2 * (tf.range(d_model, dtype=tf.float32) // 2)) / d_model)\n",
    "    angle_rads = positions * angles\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "\n",
    "def multi_head_attention(d_model, num_heads):\n",
    "    return MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)\n",
    "\n",
    "def feed_forward_network(d_model, d_ff, dropout_rate):\n",
    "    return FeedForwardNetwork(units=[d_ff, d_model], activation='relu', dropout=dropout_rate)\n",
    "\n",
    "def transformer_encoder_layer(d_model, num_heads, d_ff, dropout_rate, name='transformer_encoder'):\n",
    "    inputs = Input(shape=(None, d_model), name='input')\n",
    "    input_mask = Input(shape=(), dtype=tf.bool, name='input_mask')\n",
    "    \n",
    "    pos_encodings = positional_encoding(max_seq_len, d_model)\n",
    "    x = inputs + pos_encodings\n",
    "    \n",
    "    attn_output = multi_head_attention(d_model, num_heads)(query=x, key=x, value=x, attention_mask=input_mask)\n",
    "    attn_output = Dropout(dropout_rate)(attn_output)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + attn_output)\n",
    "    \n",
    "    ffn_output = feed_forward_network(d_model, d_ff, dropout_rate)(x)\n",
    "    ffn_output = Dropout(dropout_rate)(ffn_output)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x + ffn_output)\n",
    "    \n",
    "    encoder_model = Model(inputs=[inputs, input_mask], outputs=x, name=name)\n",
    "    return encoder_model\n",
    "\n",
    "def transformer_model(num_layers, d_model, num_heads, d_ff, input_vocab_size, max_seq_len, dropout_rate):\n",
    "    inputs = Input(shape=(None,), name='input')\n",
    "    input_mask = Input(shape=(), dtype=tf.bool, name='input_mask')\n",
    "    \n",
    "    x = Embedding(input_vocab_size, d_model)(inputs)\n",
    "    x = x * tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        x = transformer_encoder_layer(d_model, num_heads, d_ff, dropout_rate)([x, input_mask])\n",
    "    \n",
    "    transformer = Model(inputs=[inputs, input_mask], outputs=x, name='transformer')\n",
    "    return transformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"multi_head_attention_1\" (type MultiHeadAttention).\n\ndim -3 not in the interval [-2, 1]. for '{{node multi_head_attention_1/ExpandDims}} = ExpandDims[T=DT_BOOL, Tdim=DT_INT32](Placeholder_1, multi_head_attention_1/ExpandDims/dim)' with input shapes: [?], [] and with computed input tensors: input[1] = <-3>.\n\nCall arguments received by layer \"multi_head_attention_1\" (type MultiHeadAttention):\n  • query=tf.Tensor(shape=(None, 50, 128), dtype=float32)\n  • value=tf.Tensor(shape=(None, 50, 128), dtype=float32)\n  • key=tf.Tensor(shape=(None, 50, 128), dtype=float32)\n  • attention_mask=tf.Tensor(shape=(None,), dtype=bool)\n  • return_attention_scores=False\n  • training=None\n  • use_causal_mask=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m dialog_act_labels \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconvert_to_tensor(dialog_act_labels, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mint32)\n\u001b[0;32m     34\u001b[0m \u001b[39m# Build and compile the model\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m transformer \u001b[39m=\u001b[39m transformer_model(num_layers, d_model, num_heads, d_ff, vocab_size, max_seq_len, dropout_rate)\n\u001b[0;32m     37\u001b[0m optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39mlearning_rate)\n\u001b[0;32m     38\u001b[0m loss_fn \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mSparseCategoricalCrossentropy(from_logits\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[27], line 43\u001b[0m, in \u001b[0;36mtransformer_model\u001b[1;34m(num_layers, d_model, num_heads, d_ff, input_vocab_size, max_seq_len, dropout_rate)\u001b[0m\n\u001b[0;32m     40\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m*\u001b[39m tf\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39msqrt(tf\u001b[39m.\u001b[39mcast(d_model, tf\u001b[39m.\u001b[39mfloat32))\n\u001b[0;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_layers):\n\u001b[1;32m---> 43\u001b[0m     x \u001b[39m=\u001b[39m transformer_encoder_layer(d_model, num_heads, d_ff, dropout_rate)([x, input_mask])\n\u001b[0;32m     45\u001b[0m transformer \u001b[39m=\u001b[39m Model(inputs\u001b[39m=\u001b[39m[inputs, input_mask], outputs\u001b[39m=\u001b[39mx, name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtransformer\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[39mreturn\u001b[39;00m transformer\n",
      "Cell \u001b[1;32mIn[27], line 24\u001b[0m, in \u001b[0;36mtransformer_encoder_layer\u001b[1;34m(d_model, num_heads, d_ff, dropout_rate, name)\u001b[0m\n\u001b[0;32m     21\u001b[0m pos_encodings \u001b[39m=\u001b[39m positional_encoding(max_seq_len, d_model)\n\u001b[0;32m     22\u001b[0m x \u001b[39m=\u001b[39m inputs \u001b[39m+\u001b[39m pos_encodings\n\u001b[1;32m---> 24\u001b[0m attn_output \u001b[39m=\u001b[39m multi_head_attention(d_model, num_heads)(query\u001b[39m=\u001b[39;49mx, key\u001b[39m=\u001b[39;49mx, value\u001b[39m=\u001b[39;49mx, attention_mask\u001b[39m=\u001b[39;49minput_mask)\n\u001b[0;32m     25\u001b[0m attn_output \u001b[39m=\u001b[39m Dropout(dropout_rate)(attn_output)\n\u001b[0;32m     26\u001b[0m x \u001b[39m=\u001b[39m LayerNormalization(epsilon\u001b[39m=\u001b[39m\u001b[39m1e-6\u001b[39m)(x \u001b[39m+\u001b[39m attn_output)\n",
      "File \u001b[1;32mc:\\Users\\divya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\divya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1751\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1748\u001b[0m   c_op \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39mTF_FinishOperation(op_desc)\n\u001b[0;32m   1749\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mInvalidArgumentError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1750\u001b[0m   \u001b[39m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[1;32m-> 1751\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(e\u001b[39m.\u001b[39mmessage)\n\u001b[0;32m   1753\u001b[0m \u001b[39m# Record the current Python stack trace as the creating stacktrace of this\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m \u001b[39m# TF_Operation.\u001b[39;00m\n\u001b[0;32m   1755\u001b[0m \u001b[39mif\u001b[39;00m extract_traceback:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"multi_head_attention_1\" (type MultiHeadAttention).\n\ndim -3 not in the interval [-2, 1]. for '{{node multi_head_attention_1/ExpandDims}} = ExpandDims[T=DT_BOOL, Tdim=DT_INT32](Placeholder_1, multi_head_attention_1/ExpandDims/dim)' with input shapes: [?], [] and with computed input tensors: input[1] = <-3>.\n\nCall arguments received by layer \"multi_head_attention_1\" (type MultiHeadAttention):\n  • query=tf.Tensor(shape=(None, 50, 128), dtype=float32)\n  • value=tf.Tensor(shape=(None, 50, 128), dtype=float32)\n  • key=tf.Tensor(shape=(None, 50, 128), dtype=float32)\n  • attention_mask=tf.Tensor(shape=(None,), dtype=bool)\n  • return_attention_scores=False\n  • training=None\n  • use_causal_mask=False"
     ]
    }
   ],
   "source": [
    "# Define your variables\n",
    "vocab_size = 10000  # Define the size of your vocabulary\n",
    "max_seq_len = 50   # Define the maximum sequence length\n",
    "num_layers = 4  # Define the number of transformer layers\n",
    "d_model = 128  # Define the dimension of the model\n",
    "num_heads = 8  # Define the number of attention heads\n",
    "d_ff = 512  # Define the dimension of the feedforward network\n",
    "dropout_rate = 0.1  # Define the dropout rate\n",
    "learning_rate = 0.001  # Define the learning rate\n",
    "batch_size = 64  # Define the batch size\n",
    "num_epochs = 10  # Define the number of training epochs\n",
    "\n",
    "# Define and adapt the input tokenizer\n",
    "input_tokenizer = tf.keras.layers.TextVectorization(max_tokens=vocab_size, output_sequence_length=max_seq_len)\n",
    "input_tokenizer.adapt(input_sequences)\n",
    "\n",
    "# Tokenize and pad input sequences\n",
    "input_data = input_tokenizer(input_sequences)\n",
    "\n",
    "# Define and adapt the target tokenizer\n",
    "target_tokenizer = tf.keras.layers.TextVectorization(max_tokens=vocab_size, output_sequence_length=max_seq_len)\n",
    "target_tokenizer.adapt(target_sequences)\n",
    "\n",
    "# Tokenize and pad target sequences\n",
    "target_data = target_tokenizer(target_sequences)\n",
    "\n",
    "# Create masks for input sequences\n",
    "input_masks = tf.math.logical_not(tf.math.equal(input_data, 0))\n",
    "\n",
    "# Convert emotion and dialog act labels to tensors\n",
    "emotion_labels = tf.convert_to_tensor(emotion_labels, dtype=tf.int32)\n",
    "dialog_act_labels = tf.convert_to_tensor(dialog_act_labels, dtype=tf.int32)\n",
    "\n",
    "# Build and compile the model\n",
    "transformer = transformer_model(num_layers, d_model, num_heads, d_ff, vocab_size, max_seq_len, dropout_rate)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "transformer.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "# Train the model\n",
    "history = transformer.fit(\n",
    "    [input_data, input_masks],\n",
    "    target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=num_epochs,\n",
    "    validation_split=0.1\n",
    ")\n",
    "\n",
    "# Define the response generation function\n",
    "def generate_response(input_text):\n",
    "    input_tokens = input_tokenizer(input_text)\n",
    "    input_tokens = tf.expand_dims(input_tokens, 0)\n",
    "    input_mask = tf.math.logical_not(tf.math.equal(input_tokens, 0))\n",
    "    \n",
    "    predicted_tokens = transformer.predict([input_tokens, input_mask])\n",
    "    \n",
    "    # Convert predicted tokens back to text\n",
    "    predicted_text = target_tokenizer.detokenize(predicted_tokens)[0].numpy().decode('utf-8')\n",
    "    return predicted_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "338a541d02f0511409085c2eae651f4c131f886f70b3c2906ea4ac1640ce86c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
